# Uni-Navid

# 摘要

- **Uni-NaVid** 是一种**统一的视觉-语言-动作（VLA）模型**，专为**多任务具身导航**设计，支持**指令导航、目标搜索、问答交互和人物跟随**等任务。
- 模型**输入为自然语言指令和单目 RGB 视频**，**输出低级控制动作**，实现**端到端导航策略学习**。
- 引入**在线视觉 Token 合并机制**，动态压缩冗余视觉信息，**推理速度提升至 5Hz**，满足实时部署需求。
- 构建了**360 万导航样本**的大规模多任务训练集，显著增强模型的**任务协同能力与泛化能力**。
- 在多个标准导航基准上实现**SOTA 性能**，并**成功部署于真实机器人平台**，展现出优越的**实用性与鲁棒性**。